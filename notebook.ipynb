```python
{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Generate real estate descriptions and save to CSV\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Load CSV file with real estate data\n",
        "def load_data(file_path):\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Lists for varied description phrasing\n",
        "bedroom_phrases = [\"spacious bedrooms\", \"cozy bedrooms\", \"well-appointed bedrooms\"]\n",
        "bathroom_phrases = [\"modern bathrooms\", \"elegant bathrooms\", \"functional bathrooms\"]\n",
        "lot_phrases = [\"ample outdoor space\", \"generous lot\", \"room for activities\"]\n",
        "value_phrases = [\"excellent deal\", \"fantastic value\", \"priced to sell\"]\n",
        "appeal_phrases = [\"perfect for families\", \"ideal for investors\", \"great for urban living\"]\n",
        "city_context = {\n",
        "    \"Adjuntas\": \"lush greenery and mountain views\",\n",
        "    \"Juana Diaz\": \"rich cultural heritage\",\n",
        "    \"Ponce\": \"historic architecture\",\n",
        "    \"Mayaguez\": \"lively arts scene\",\n",
        "    \"Richland\": \"scenic Columbia River\"\n",
        "}\n",
        "\n",
        "# Estimate house size if missing (based on bedrooms and bathrooms)\n",
        "def estimate_house_size(row):\n",
        "    if pd.isna(row['house_size']):\n",
        "        bed = row['bed'] if not pd.isna(row['bed']) else 2\n",
        "        bath = row['bath'] if not pd.isna(row['bath']) else 1\n",
        "        return bed * 300 + bath * 150  # Approx. 300 sq ft/bedroom, 150 sq ft/bathroom\n",
        "    return row['house_size']\n",
        "\n",
        "# Generate a descriptive text for each property\n",
        "def generate_description(row):\n",
        "    price = row['price'] if not pd.isna(row['price']) else \"unknown\"\n",
        "    bed = int(row['bed']) if not pd.isna(row['bed']) else \"unknown\"\n",
        "    bath = int(row['bath']) if not pd.isna(row['bath']) else \"unknown\"\n",
        "    acre_lot = row['acre_lot'] if not pd.isna(row['acre_lot']) else \"unknown\"\n",
        "    house_size = estimate_house_size(row)\n",
        "    city = row['city'] if not pd.isna(row['city']) else \"the city\"\n",
        "    state = row['state'] if not pd.isna(row['state']) else \"the state\"\n",
        "\n",
        "    # Randomly select phrases for variety\n",
        "    bedroom_phrase = random.choice(bedroom_phrases)\n",
        "    bathroom_phrase = random.choice(bathroom_phrases)\n",
        "    lot_phrase = random.choice(lot_phrases)\n",
        "    value_phrase = random.choice(value_phrases) if price != \"unknown\" and price < 100000 else \"competitive\"\n",
        "    appeal_phrase = random.choice(appeal_phrases)\n",
        "\n",
        "    # Build description\n",
        "    description = f\"For sale in {city}, {state}, this property is a unique opportunity. \"\n",
        "    description += f\"Located in {city}, {city_context.get(city, 'a welcoming community')}, it’s a gem. \"\n",
        "    if bed != \"unknown\":\n",
        "        description += f\"Features {bed} {bedroom_phrase}, \"\n",
        "    if bath != \"unknown\":\n",
        "        description += f\"and {bath} {bathroom_phrase}. \"\n",
        "    description += f\"Spans ~{house_size:,.0f} sq ft. \"\n",
        "    if acre_lot != \"unknown\":\n",
        "        description += f\"On a {acre_lot:.2f}-acre lot with {lot_phrase}. \"\n",
        "    if price != \"unknown\":\n",
        "        description += f\"Listed at ${price:,.2f}, it’s {value_phrase}. \"\n",
        "    description += f\"{appeal_phrase.capitalize()} in {city}.\"\n",
        "\n",
        "    return description\n",
        "\n",
        "# Process CSV and save with descriptions\n",
        "def process_data(file_path, output_path):\n",
        "    data = load_data(file_path)\n",
        "    data['description'] = data.apply(generate_description, axis=1)\n",
        "    output_data = data[['status', 'price', 'bed', 'bath', 'acre_lot', 'city', 'state', 'house_size', 'description']]\n",
        "    output_data.to_csv(output_path, index=False)\n",
        "    print(f\"Descriptions saved to {output_path}\")\n",
        "\n",
        "# Run the processing\n",
        "input_file = \"realtor-data.zip.csv\"\n",
        "output_file = \"realtor-data-with-descriptions.csv\"\n",
        "process_data(input_file, output_file)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Load and split data for model training\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load descriptions from CSV\n",
        "data_path = \"realtor-data-with-descriptions.csv\"\n",
        "descriptions = pd.read_csv(data_path)['description'].dropna().tolist()\n",
        "\n",
        "# Split data into train (80%), validation (10%), and test (10%) sets\n",
        "train_texts, temp_texts = train_test_split(descriptions, test_size=0.2, random_state=42)\n",
        "val_texts, test_texts = train_test_split(temp_texts, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Datasets\n",
        "train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
        "val_dataset = Dataset.from_dict({\"text\": val_texts})\n",
        "test_dataset = Dataset.from_dict({\"text\": test_texts})\n",
        "\n",
        "print(f\"Training samples: {len(train_texts)}\")\n",
        "print(f\"Validation samples: {len(val_texts)}\")\n",
        "print(f\"Test samples: {len(test_texts)}\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Load model and tokenizer\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"google/gemma-3-1b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# Set padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "print(\"Model and tokenizer loaded!\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Tokenize datasets\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "# Tokenize function to prepare data for training\n",
        "def tokenize_function(examples):\n",
        "    encodings = tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=256)\n",
        "    encodings[\"labels\"] = encodings[\"input_ids\"].copy()\n",
        "    return encodings\n",
        "\n",
        "# Tokenize datasets\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Set format for PyTorch\n",
        "train_dataset.set_format(\"torch\")\n",
        "val_dataset.set_format(\"torch\")\n",
        "test_dataset.set_format(\"torch\")\n",
        "\n",
        "print(f\"Tokenized datasets ready! Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 5: Apply LoRA configuration\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Define LoRA configuration for efficient fine-tuning\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Low-rank matrix rank\n",
        "    lora_alpha=32,  # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Transformer modules to adapt\n",
        "    lora_dropout=0.1,  # Dropout for regularization\n",
        "    task_type=\"CAUSAL_LM\"  # Causal language modeling\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(\"LoRA applied to model!\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 6: Set up and run training\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import torch\n",
        "\n",
        "# Define custom collate function for batch processing\n",
        "def custom_collate_fn(batch):\n",
        "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
        "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
        "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"model_output\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=1000,\n",
        "    save_steps=1000,\n",
        "    logging_steps=200,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\"\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=custom_collate_fn\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the final model\n",
        "trainer.save_model(\"final_model\")\n",
        "print(\"Training completed! Model saved to final_model\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 7: Streamlit UI for real estate queries\n",
        "\n",
        "import streamlit as st\n",
        "from transformers import pipeline\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Set Streamlit page configuration\n",
        "st.set_page_config(page_title=\"Real Estate Expert\", page_icon=\"🏠\")\n",
        "\n",
        "# Load fine-tuned model\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    base_model_name = \"google/gemma-3-1b-it\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "    lora_model = PeftModel.from_pretrained(model, \"final_model\", torch_dtype=torch.float16)\n",
        "    return tokenizer, lora_model\n",
        "\n",
        "tokenizer, lora_model = load_model()\n",
        "\n",
        "# Set up text generation pipeline\n",
        "text_gen = pipeline(\"text-generation\", model=lora_model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "\n",
        "# Define system prompt for real estate expertise\n",
        "system_prompt = (\n",
        "    \"You are a U.S. real estate expert specializing in property price evaluation. \"\n",
        "    \"You provide detailed, data-driven price assessments based on location, property condition, and market trends.\"\n",
        ")\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"🏠 Real Estate Expert\")\n",
        "st.markdown(\"Ask about property price evaluations or market trends.\")\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat history\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Handle user input\n",
        "if prompt := st.chat_input(\"Enter your real estate question\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    \n",
        "    # Generate response\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Analyzing...\"):\n",
        "            full_prompt = f\"{system_prompt}\\nUser: {prompt}\\nAssistant:\"\n",
        "            response = text_gen(full_prompt, max_new_tokens=200, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
        "            answer = response.split(\"Assistant:\")[-1].strip()\n",
        "            st.markdown(answer)\n",
        "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n"
      ],
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
```
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
